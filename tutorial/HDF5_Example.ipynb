{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668b0e12",
   "metadata": {},
   "source": [
    "# Author & Notes\n",
    "\n",
    "This Jupyter Notebook was written by Benjamin S. Meyers ([bsm9339@rit.edu](mailto:bsm9339@rit.edu)). This is a very simple tutorial to get your started on working with HDF5. This covers datasets, but HDF5 is more than just data storage, it's also a set of functions for working with that data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d12d44",
   "metadata": {},
   "source": [
    "# Background on HDF5\n",
    "\n",
    "[HDF5](https://www.hdfgroup.org/solutions/hdf5/) has been completely revamped since HDF4 (sort of like Python 2.7 vs. Python 3+). HDF5 has seven _concepts_ that you need to worry about:\n",
    "- **File:** The HDF5 file containing any number of datasets. Typically organized like a file directory (see Groups).\n",
    "- **Dataset:** Datasets contain data in the form of an n-dimensional array, typically with elements of the same type.\n",
    "- **Datatype:** Metadata describing the individual elements of the dataset, e.g. 32bit integers, structs, strings, etc.\n",
    "- **Dataspace:** Metadata describing the layout of the data, e.g. 3-D array.\n",
    "- **Attribute:** Simple metadata describing basically anything you want.\n",
    "- **Group:** A group is a collection of related datasets. Starting from the root group of the file, you may have a group of visualizations and a group of datasets, for example.\n",
    "- **Link:** Links between related objects (Datasets and Groups) inside an HDF5 file (or even in another HDF5 file).\n",
    "\n",
    "As you can see, HDF5 is object-oriented in it's design, but it's actually implemented in C for the sake of efficiency.\n",
    "\n",
    "Here is a very good [video tutorial](https://www.youtube.com/watch?v=BAjsCldRMMc) to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652df6e",
   "metadata": {},
   "source": [
    "# Background on H5py\n",
    "\n",
    "We'll be using [H5py](https://docs.h5py.org/en/stable/index.html), which is a popular Python wrapper for HDF5. [PyTables](https://www.pytables.org/) is an alternative, but they have a \"there is only one good way to do X\" mindset, so they're a bit limited.\n",
    "\n",
    "The key thing to remember when using H5py is that HDF5 groups work like dictionaries, and HDF5 datasets work like NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f58dd",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Install HDF5:\n",
    "- Debian/Ubuntu: `sudo apt install libhdf5-dev`\n",
    "- Others: [See Documentation](https://www.hdfgroup.org/downloads/hdf5)\n",
    "\n",
    "Install H5py: `pip3 install h5py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee097f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe63b40",
   "metadata": {},
   "source": [
    "# (1) Creating an HDF5 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de174f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"testfile.hdf5\", \"w\") as f:\n",
    "    # This creates a file called `testfile.hdf5` containing a dataset called\n",
    "    # `test_dataset` with a 1-D array of size 100 with integer elements\n",
    "    test_dataset = f.create_dataset(\"test_dataset\", (100, ), dtype=\"i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcad53d",
   "metadata": {},
   "source": [
    "NOTE: If you try to view `testfile.hdf5` in vim or some other editor, you'll only see compiled code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd7ad2",
   "metadata": {},
   "source": [
    "# (2) Reading, Writing, and Closing an HDF5 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a23d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the read permissions\n",
    "with h5py.File(\"testfile.hdf5\", \"r\") as f:\n",
    "    pass # Do stuff\n",
    "# OR\n",
    "f = h5py.File(\"testfile.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41fe83f",
   "metadata": {},
   "source": [
    "So, how do we know what we have inside this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416aab87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dataset']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025533c8",
   "metadata": {},
   "source": [
    "Now we know this file contains a dataset called \"test_dataset\". Let's create another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf161b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create dataset (no write intent on file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-919fb490b8ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Variable length unicode strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mdset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create dataset (no write intent on file)"
     ]
    }
   ],
   "source": [
    "# Variable length unicode strings\n",
    "dt = h5py.special_dtype(vlen=str)\n",
    "f.create_dataset(\"apples\", (10,), dtype=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621abbdf",
   "metadata": {},
   "source": [
    "We opened the HDF5 file with read permissions, so we can't modify it. Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027ac13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dataset']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always do this\n",
    "h5py.File.close(f)\n",
    "# Not this\n",
    "# f.close()\n",
    "\n",
    "# \"a\" = \"rw\" if the file exists\n",
    "f = h5py.File(\"testfile.hdf5\", \"a\")\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5713f5c",
   "metadata": {},
   "source": [
    "**Read that again.** If you run `f.close()` instead of `h5py.File.close(f)`, your HDF5 will likely be corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8222d1",
   "metadata": {},
   "source": [
    "# (3) Modifying and Accessing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deab5e4",
   "metadata": {},
   "source": [
    "Okay, now let's try creating another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0746769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable length unicode strings\n",
    "dt = h5py.special_dtype(vlen=str)\n",
    "apples_dataset = f.create_dataset(\"apples\", (10,), dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9df77ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'test_dataset']\n",
      "(10,)\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(list(f.keys()))\n",
    "print(apples_dataset.shape)\n",
    "print(apples_dataset.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59d51b",
   "metadata": {},
   "source": [
    "We've got two datasets. Let's add some apples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48737146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Red Delicious' 'Gala' 'Granny Smith' 'Golden Delicious' 'Lady' 'Baldwin'\n",
      " 'McIntosh' 'Honey Crisp' 'Fuji' 'Cortland']\n",
      "[b'', b'', b'', b'', b'', b'', b'', b'', b'', b'']\n"
     ]
    }
   ],
   "source": [
    "apples = [\"Red Delicious\", \"Gala\", \"Granny Smith\", \"Golden Delicious\", \"Lady\", \"Baldwin\", \"McIntosh\", \"Honey Crisp\", \"Fuji\", \"Cortland\"]\n",
    "apples_dataset = np.array(apples, dtype=dt)\n",
    "print(apples_dataset)\n",
    "print(list(f[\"apples\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0d88f",
   "metadata": {},
   "source": [
    "So, setting the pointer `apples_dataset` to a NumPy array **does not** change the actually dataset. Instead, we need to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28c200af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Red Delicious', b'Gala', b'Granny Smith', b'Golden Delicious', b'Lady', b'Baldwin', b'McIntosh', b'Honey Crisp', b'Fuji', b'Cortland']\n"
     ]
    }
   ],
   "source": [
    "apples_dataset = f[\"apples\"]\n",
    "# The `[...]` is critical\n",
    "apples_dataset[...] = np.array(apples, dtype=dt)\n",
    "print(list(f[\"apples\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007e71b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Red Delicious'\n"
     ]
    }
   ],
   "source": [
    "print(apples_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8469a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index (10) out of range for (0-9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dcd59e9c3955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This should give us an IndexError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mapples_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Empire\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;31m# Perform the dataspace selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnselect\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/selections.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(shape, args, dataset)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_selector.pyx\u001b[0m in \u001b[0;36mh5py._selector.Selector.make_selection\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_selector.pyx\u001b[0m in \u001b[0;36mh5py._selector.Selector.apply_args\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Index (10) out of range for (0-9)"
     ]
    }
   ],
   "source": [
    "# This should give us an IndexError\n",
    "apples_dataset[10] = \"Empire\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a5308",
   "metadata": {},
   "source": [
    "If we want to add more data, we need to change the shape of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b784fb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only chunked datasets can be resized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1613d1132ada>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapples_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapples_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, axis)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only chunked datasets can be resized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Only chunked datasets can be resized"
     ]
    }
   ],
   "source": [
    "apples_dataset.resize((11,))\n",
    "print(apples_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10229b11",
   "metadata": {},
   "source": [
    "Alright, so we didn't plan very well. We need our dataset to be _chunked_ if we want to resize it. To understand what this means, we need to look at how HDF5 stores data:\n",
    "- **Contiguous Layout:** The default. Datasets are serialized into a monolithic block, which maps directly to a memory buffer the size of the dataset.\n",
    "- **Chunked Layout:**: Datasets are split into chunks which are stored separately in the file. Storage order doesn't matter. The benefit of chunking is that (1) datasets can be resized, and (2) chunks can be read/written individually, improving performance when manipulating a subset of the dataset. [More details](https://support.hdfgroup.org/HDF5/doc/Advanced/Chunking/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c695f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Red Delicious', b'Gala', b'Granny Smith', b'Golden Delicious', b'Lady', b'Baldwin', b'McIntosh', b'Honey Crisp', b'Fuji', b'Cortland']\n"
     ]
    }
   ],
   "source": [
    "# Delete our \"apples\" dataset\n",
    "del f[\"apples\"]\n",
    "\n",
    "# Recreate it, but make it chunked\n",
    "apples_dataset = f.create_dataset(\"apples\", (10,), maxshape=(None,), dtype=dt, chunks=True)\n",
    "apples_dataset[...] = np.array(apples, dtype=dt)\n",
    "print(list(f[\"apples\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacd5aa",
   "metadata": {},
   "source": [
    "Now we should be able to resize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eaf70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"apples\": shape (11,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "apples_dataset.resize((11,))\n",
    "print(apples_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cadc28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Red Delicious', b'Gala', b'Granny Smith', b'Golden Delicious', b'Lady', b'Baldwin', b'McIntosh', b'Honey Crisp', b'Fuji', b'Cortland', b'Empire']\n"
     ]
    }
   ],
   "source": [
    "apples_dataset[10] = \"Empire\"\n",
    "print(list(f[\"apples\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154a506",
   "metadata": {},
   "source": [
    "# (4) Attrributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b2a8613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(dict(apples_dataset.attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d72d6",
   "metadata": {},
   "source": [
    "So, our \"apples\" dataset has no attributes, i.e. no metadata. Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d318e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_name = \"Description\"\n",
    "attr_data = \"List of the most popular species of apples.\"\n",
    "apples_dataset.attrs.create(name=attr_name, data=attr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69c635f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Description': 'List of the most popular species of apples.'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(apples_dataset.attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd873d",
   "metadata": {},
   "source": [
    "# (5) Groups\n",
    "\n",
    "Groups allow us to group related datasets together. So let's make a group for vegetable datasets. First, let's see what the existing group hierarachy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c86c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7060e47",
   "metadata": {},
   "source": [
    "That's the root group. What about our apples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e06dab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apples\n"
     ]
    }
   ],
   "source": [
    "print(f[\"apples\"].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74354b6c",
   "metadata": {},
   "source": [
    "And now some veggies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c2dd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "veggie_group = f.create_group(\"vegetables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f20beddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/vegetables\" (0 members)>\n"
     ]
    }
   ],
   "source": [
    "print(veggie_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a0377",
   "metadata": {},
   "source": [
    "Now we need some datasets for our veggie group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa3cfd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"root_veggies\": shape (10,), type \"|O\">\n",
      "<HDF5 dataset \"leafy_veggies\": shape (10,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "root_veggies = veggie_group.create_dataset(\"root_veggies\", (10,), maxshape=(None,), dtype=dt, chunks=True)\n",
    "print(root_veggies)\n",
    "leafy_veggies = veggie_group.create_dataset(\"leafy_veggies\", (10,), maxshape=(None,), dtype=dt, chunks=True)\n",
    "print(leafy_veggies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "998db4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Onions', b'Sweet Potatoes', b'Turnips', b'Ginger', b'Beets', b'Garlic', b'Radishes', b'Turnips', b'Fennel', b'Carrots']\n"
     ]
    }
   ],
   "source": [
    "root_veggies[...] = np.array([\"Onions\", \"Sweet Potatoes\", \"Turnips\", \"Ginger\", \"Beets\", \"Garlic\", \"Radishes\", \"Turnips\", \"Fennel\", \"Carrots\"])\n",
    "print(list(root_veggies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d2b729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vegetables/root_veggies\n",
      "/vegetables/leafy_veggies\n",
      "/vegetables\n"
     ]
    }
   ],
   "source": [
    "print(root_veggies.name)\n",
    "print(leafy_veggies.name)\n",
    "print(veggie_group.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37531b53",
   "metadata": {},
   "source": [
    "As you can see, groups are basically directories of related datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba8068fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apples': <HDF5 dataset \"apples\": shape (11,), type \"|O\">, 'test_dataset': <HDF5 dataset \"test_dataset\": shape (100,), type \"<i4\">, 'vegetables': <HDF5 group \"/vegetables\" (2 members)>}\n"
     ]
    }
   ],
   "source": [
    "print(dict(f.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4272059",
   "metadata": {},
   "source": [
    "We can iterate over the items in a group and run a function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9715007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leafy_veggies\n",
      "root_veggies\n"
     ]
    }
   ],
   "source": [
    "def printname(name):\n",
    "    print(name)\n",
    "f[\"vegetables\"].visit(printname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe66dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
